---
title: "Data Wrangling"
format: html
#code-fold: true
editor: visual
---

This chapter walks through the most common steps involved in taking raw data and turning it into analysis-ready data. You’ll learn how to import datasets, clean and transform variables, handle missing values, combine multiple data sources, and filter data to answer specific questions. Along the way, we’ll introduce essential R data-management functions-mostly from the **tidyverse**-and show how to chain them together using pipes to create clear, readable workflows.

## Loading data from a file

Often your data will be stored in a file that you need to load into R. Common examples include:

| File type              | Extension | Common function | Package    |
|------------------------|-----------|-----------------|------------|
| Excel spreadsheet      | `.xlsx`   | `read_excel()`  | **readxl** |
| Comma-separated values | `.csv`    | `read_csv()`    | **readr**  |
| Stata dataset          | `.dta`    | `read_dta()`    | **haven**  |

*Note*: You can use read_csv() and read_excel() after loading tidyverse. You can also use **rio** package, which has an `import()` function that works for just about any data file type you might want.

```{r}
library(tidyverse)

water <- read_csv("../data/water_data.csv")

#water <- read_csv("water_data.csv")

ls(water)
```

## Looking at the data

```{r}
class(water)
head(water)
water %>% glimpse()
summary(water)
nrow(water)
ncol(water)
```

```{r}
# type of variables in water
class(water$year)
unique(water$year)
```

## Cleaning variable names

```{r}
water <- water %>%
  rename(county_code = county) #rename
```

## Selecting and filtering data

```{r}
# Create a subset with only two variables: year and county_code
sub1 <- water %>%
  select(year, county_code)

# Get rid of the field variable
sub2 <- water %>%
  select(-field)

# Create a subset containing only year=2000
sub.2000 <- water %>%
  filter(year == 2000)
```

## Creating variables

```{r}
water <- water %>%
  mutate(year2 = year + 1,
         year3 = year2 + 3) # takes our year variable and adds 1 to it

# create a subset center_pivot == 1 and create a new variable = total cost
cost <- water %>%
  filter(center_pivot == 1) %>%
  mutate(tot.cost = price_water*q_water)

options(scipen = 999)
summary(cost$tot.cost)
```

## Aggregating data

A dataset’s observation level is simply what each row of the data represents. For example, `water` dataset contains one row per field for a given year, then the observation level is field-year.

Each row corresponds to one field in one specific year. Sometimes your data are more detailed than what you need for your analysis, and you may want to zoom out. For instance, you might start with field-year data but want to analyze outcomes at the field level overall. To do that, you need to aggregate the data.

In the tidyverse, this is usually done with `group_by()`, which tells R to perform calculations separately within each group. You’ll then typically use `summarize()` to combine multiple rows into a single row per group. The resulting dataset will have an observation level based on whatever variable(s) you used in `group_by()`.

```{r}
# get average total water cost by field
# averaged over all the years
avg.cost <- cost %>%
  group_by(field) %>%
  summarize(avg.cost = mean(tot.cost)) # add sum
```

## Multi-row calculations

It’s pretty common to need to use more than one row of data (but not the entire dataset) to calculate something you care about. For example, to calculate statistics by group, we can use `group_by()` to do by-group calculations. However, if we want to calculate by group and create a new column with that calculation rather than change the observation level, we want to follow that `group_by()` with a `mutate()` instead of a summarize.

```{r}
cost <- cost %>%
  group_by(field) %>%
  mutate(avg.cost = mean(tot.cost, na.rm = T),
         tot.use = sum(q_water))

# What if I need the avg cost and total use by field-year?
```

```{r}
# Your time to practice
# 1) Create a subset that includes only years > 1999
# 2) create a new variable: q_water10 = (q_water + 10)
# 3) create a two new variables: min.use and min.price by year

```

```{r}
data1 <- water %>%
  filter(year > 1999) %>%
  mutate(q_water10 = q_water + 10) %>%
  group_by(year) %>%
  mutate(min.use = min(q_water),
         min.price = min(price_water))
```

## Re-code values using logic and conditions

**Specific values**

To re-code with simple logical criteria, you can use `replace()` within mutate().

```{r}
# avg cost is changed for field = 24

avg.cost <- avg.cost %>%
  mutate(avg.cost = replace(avg.cost, field == 24, 8.564))
```

Another function for simple logic is `ifelse()`. It applies a simple condition to a whole vector at once and return one value if the condition is true and another if it’s false. It’s especially useful for creating or transforming variables based on rules.

Let's create a new variable, `type`, that takes the value 1 when `q_water`\> mean(q_water), and 0 otherwise.

```{r}
# calculate the mean of q_water
summary(water$q_water)

water <- water %>%
  mutate(type = ifelse(q_water > 14.05, 1, 0))

class(water$type)

# Convert type to factor variable and create labels.
water <- water %>%
  mutate(type = factor(type, levels = c(0, 1), labels = c("low", "high")))

?factor
class(water$type)
```

**Complex logic**

Use `case_when()` when you need to recode a variable into many groups or apply more complex logical rules. The function checks each row of the data and assigns a new value based on the conditions you specify.

Each `case_when()` rule has a condition on the left and a value on the right, separated by a tilde (\~):

-   Conditions go on the left-hand side (LHS)
-   Assigned values go on the right-hand side (RHS)
-   Rules are separated by commas

Now, we want to create `type2` but taking three values, low, medium and high.

```{r}
summary(water$q_water)

water <- water %>%
  mutate(type2 = case_when(
    q_water < 12 ~ 0,
    q_water >= 12 & q_water < 15 ~ 1,
    q_water >= 15 ~ 2
  ))

# using ifelse
water <- water %>%
  mutate(type.test = ifelse(q_water < 12, 0, ifelse(q_water >= 12 & q_water < 15, 1,2)))

# Convert type to factor variable and create labels.
water <- water %>%
  mutate(type_fac = factor(type2, levels = c(0, 1, 2), labels = c("low", "medium", "high")))

# or create a factor variable directly
water <- water %>%
  mutate(type2 = factor(case_when(
    q_water < 12 ~ 0,
    q_water >= 12 & q_water < 15 ~ 1,
    q_water >= 15 ~ 2),
    levels = c(0, 1, 2), 
    labels = c("low", "medium", "high")))
```

## Missing values

When we are cleaning our data, we need to handle missing values. They are represented by special values: `NA`, `NULL`, `NaN` and `Inf`.

**Examples**

| R command | Outcome |
|-----------|---------|
| `5 / 0`   | Inf     |
| `0 / 0`   | NaN     |
| `5 / NA`  | NA      |

**Useful functions**

Let's build an example dataset and use some important functions to deal with missing values.

```{r}
df <- tibble(
  id     = 1:10,
  score  = c(10, 9, NA, 7, 6, NA, 8, 9, NA, 5),
  group  = c("A", "A", "A", "B", "B", "B", "A", "A", "B", "B"))

df
```

Detect missing values `is.na()`

```{r}
df %>%
  mutate(score_na = is.na(score))

is.na(df)
```

Keep only complete values `!is.na()`, `drop_na()`

```{r}
df_comp <- df %>%
  filter(!is.na(score))

df_comp <- df %>%
  drop_na(score)
```

Ignore missing values in calculations `na.rm = TRUE`

```{r}
df %>% 
  summarise(mean_socre = mean(score))

df %>% 
  summarise(mean_socre = mean(score, na.rm = T))
```

*Your turn to practice*: calculate the average score by group.

```{r}
df %>%
  group_by(group) %>%
  summarise(group_socre = mean(score, na.rm = T))

# if you want to save the outcome
group_socre <- df %>%
  group_by(group) %>%
  summarise(group_socre = mean(score, na.rm = T))
```

## Joining data

Often, you’ll work with multiple datasets that describe the same observations (for example, the same people, places, or time periods). To combine these datasets, you use a merge (or join) based on one or more key variables that appear in both datasets. These key variables act like an ID that tells R which rows belong together. At least one of the datasets should have only one row per key value, so R knows exactly how to match the observations without ambiguity.

```{r}
# let's create a new dataset to merge with water
water2 <- water %>%
  select(year, field) %>% # IDs variables
  mutate(
    x = rnorm(n()),
    y = rnorm(n()))
```

We now have a second dataset that includes variables `x` and `y`, and we want to bring those variables back into our original `water` dataset. To do this, we use `joins` functions, which are provided by the **dplyr** package (included in the **tidyverse**).

Join functions can be used in two ways: as standalone commands that create a new data frame, or inside a pipe (`%>%`) as part of a larger data-cleaning workflow.

In this example, we use `left_join()` as a standalone command to create a new data frame called `joined_data`. The first data frame listed (`water`) is the baseline dataset we want to keep, and the second data frame (`water.2`) is joined to it.

The `by =` argument tells R which columns to use to match rows between the two datasets. These columns must exist in both data frames and should uniquely identify the observations so the rows line up correctly.

```{r}
joined_data <- left_join(water, water2, by = c("year", "field")) # note we use two IDs variables to uniquely identify the observations
```

**Left and right joins**

`left_join()`, `right_join()`

Left and right joins are commonly used to add new information to an existing dataset.

The order of the datasets matters:

-   In a left join, the first dataset is the baseline.
-   In a right join, the second dataset is the baseline.

All rows from the baseline dataset are kept. Data from the other dataset is added only when there is a match on the identifier column(s). Rows in the secondary dataset that do not match are dropped.

If one row in the secondary dataset matches multiple baseline rows, the same information is added to each matching row. If a baseline row matches multiple rows in the secondary dataset, new rows will be created, increasing the size of the data.

*Should we use a left join or a right join?* To answer this question, ask yourself: Which dataset do I want to keep all rows from?

![Source: Datacamp](pics/left_join.png)

![Source: Datacamp](pics/right_join.png)

**Full joins**

`full_join()`

A full join is the most inclusive type of join. It keeps all rows from both datasets.

If a row appears in one dataset but not the other, it is still included in the result. Any missing information created by these unmatched rows is filled in with NA. Because full joins can increase both the number of rows and columns, it’s a good idea to keep an eye on the output to catch issues like mismatched variable names, case sensitivity, or small differences in character values.

In a full join, the dataset listed first in the command is treated as the baseline. While this does not change which rows are returned, it can affect the order of rows and columns and which identifier columns are kept.

![Source: Datacamp](pics/full_join.png)

**Inner join**

`inner_join()`

An inner join is the most restrictive type of join—it keeps only rows that have matching values in both datasets.

Because of this, the number of rows in the resulting dataset may be smaller than in the original (baseline) dataset.

![Source: Datacamp](pics/inner_join.png)
